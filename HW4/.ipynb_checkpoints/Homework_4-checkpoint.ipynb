{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Jm3UN_Hfsu"
   },
   "source": [
    "## **Homework 4**\n",
    "**Instructions**\n",
    "* This homework focuses on understanding and applying CoCoOp for CLIP prompt tuning. It consists of **four questions** designed to assess both theoretical understanding and practical application.\n",
    "\n",
    "* Please organize your answers and results for the questions below and submit this jupyter notebook as **a .pdf file**.\n",
    "\n",
    "* **Deadline: 11/26 (Sat) 23:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeRABv42Ku4E"
   },
   "source": [
    "### **Preparation**\n",
    "\n",
    "* Run the code below before proceeding with the homework (Q1, Q2).\n",
    "* If an error occurs, click ‘Run Session Again’ and then restart the runtime from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNOsgBEzKucv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'ProMetaR'...\n",
      "remote: Enumerating objects: 129, done.\u001B[K\n",
      "remote: Counting objects: 100% (129/129), done.\u001B[K\n",
      "remote: Compressing objects: 100% (85/85), done.\u001B[K\n",
      "remote: Total 129 (delta 37), reused 101 (delta 24), pack-reused 0 (from 0)\u001B[K\n",
      "Receiving objects: 100% (129/129), 2.90 MiB | 2.86 MiB/s, done.\n",
      "Resolving deltas: 100% (37/37), done.\n",
      "/Users/greatpark/Desktop/Studies/3-2/Deep Learning/Homework/HW4/ProMetaR\n",
      "Cloning into 'Dassl.pytorch'...\n",
      "remote: Enumerating objects: 2477, done.\u001B[K\n",
      "remote: Counting objects: 100% (993/993), done.\u001B[K\n",
      "remote: Compressing objects: 100% (288/288), done.\u001B[K\n",
      "remote: Total 2477 (delta 777), reused 861 (delta 705), pack-reused 1484 (from 1)\u001B[K\n",
      "Receiving objects: 100% (2477/2477), 428.00 KiB | 2.76 MiB/s, done.\n",
      "Resolving deltas: 100% (1658/1658), done.\n",
      "/Users/greatpark/Desktop/Studies/3-2/Deep Learning/Homework/HW4/ProMetaR/Dassl.pytorch\n",
      "Collecting flake8==3.7.9 (from -r requirements.txt (line 1))\n",
      "  Obtaining dependency information for flake8==3.7.9 from https://files.pythonhosted.org/packages/f8/1f/7ea40d1e4146ea55dbab41cda1376db092a75794914169aabd7e8d7a7def/flake8-3.7.9-py2.py3-none-any.whl.metadata\n",
      "  Downloading flake8-3.7.9-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting yapf==0.29.0 (from -r requirements.txt (line 2))\n",
      "  Obtaining dependency information for yapf==0.29.0 from https://files.pythonhosted.org/packages/7c/21/534d143afd3df9cae9b21674fcc32207cb80cfb3de56b89ef7a37c746cca/yapf-0.29.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading yapf-0.29.0-py2.py3-none-any.whl.metadata (30 kB)\n",
      "Collecting isort==4.3.21 (from -r requirements.txt (line 3))\n",
      "  Obtaining dependency information for isort==4.3.21 from https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl.metadata\n",
      "  Downloading isort-4.3.21-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting yacs (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for yacs from https://files.pythonhosted.org/packages/38/4f/fe9a4d472aa867878ce3bb7efb16654c5d63672b86dc0e6e953a67018433/yacs-0.1.8-py3-none-any.whl.metadata\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting gdown (from -r requirements.txt (line 5))\n",
      "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/54/70/e07c381e6488a77094f04c85c9caf1c8008cdc30778f7019bc52e5285ef0/gdown-5.2.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tb-nightly (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for tb-nightly from https://files.pythonhosted.org/packages/92/dc/03a6d629c721ed1131be9588b6e62291cc5c89320d9f65e782e624c476a8/tb_nightly-2.19.0a20241121-py3-none-any.whl.metadata\n",
      "  Downloading tb_nightly-2.19.0a20241121-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: future in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.18.3)\n",
      "Requirement already satisfied: scipy in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: tqdm in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (4.65.0)\n",
      "Collecting ftfy (from -r requirements.txt (line 11))\n",
      "  Obtaining dependency information for ftfy from https://files.pythonhosted.org/packages/ab/6e/81d47999aebc1b155f81eca4477a616a70f238a2549848c38983f3c22a82/ftfy-6.3.1-py3-none-any.whl.metadata\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (2022.7.9)\n",
      "Collecting wilds==1.2.2 (from -r requirements.txt (line 13))\n",
      "  Obtaining dependency information for wilds==1.2.2 from https://files.pythonhosted.org/packages/95/40/7cd1716814fed530ec835de09217593940e391d90772cb84de67435f0ceb/wilds-1.2.2-py3-none-any.whl.metadata\n",
      "  Downloading wilds-1.2.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: tabulate in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.8.10)\n",
      "Collecting entrypoints<0.4.0,>=0.3.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for entrypoints<0.4.0,>=0.3.0 from https://files.pythonhosted.org/packages/ac/c6/44694103f8c221443ee6b0041f69e2740d89a25641e62fb4f2ee568f2f9c/entrypoints-0.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading entrypoints-0.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyflakes<2.2.0,>=2.1.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for pyflakes<2.2.0,>=2.1.0 from https://files.pythonhosted.org/packages/84/f2/ed0ffb887f8138a8fe5a621b8c0bb9598bfb3989e029f6c6a85ee66628ee/pyflakes-2.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pyflakes-2.1.1-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pycodestyle<2.6.0,>=2.5.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for pycodestyle<2.6.0,>=2.5.0 from https://files.pythonhosted.org/packages/0e/0c/04a353e104d2f324f8ee5f4b32012618c1c86dd79e52a433b64fceed511b/pycodestyle-2.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading pycodestyle-2.5.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0 (from flake8==3.7.9->-r requirements.txt (line 1))\n",
      "  Obtaining dependency information for mccabe<0.7.0,>=0.6.0 from https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (1.23.5)\n",
      "Collecting ogb>=1.2.6 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Obtaining dependency information for ogb>=1.2.6 from https://files.pythonhosted.org/packages/7e/95/e0770cf1ad9667492f56b732f44398ef2756d61df914e10d121a3cad013a/ogb-1.3.6-py3-none-any.whl.metadata\n",
      "  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting outdated>=0.2.0 (from wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Obtaining dependency information for outdated>=0.2.0 from https://files.pythonhosted.org/packages/d3/04/7d2b9a0d1b81e30f39e6f358bac01f4f18b585f35b0ffc5c83fc274f146b/outdated-0.2.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.0.3)\n",
      "Requirement already satisfied: pillow>=7.2.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (9.4.0)\n",
      "Requirement already satisfied: pytz>=2020.4 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2022.7)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (2.0.0)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from wilds==1.2.2->-r requirements.txt (line 13)) (0.15.1)\n",
      "Requirement already satisfied: PyYAML in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from yacs->-r requirements.txt (line 4)) (6.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from gdown->-r requirements.txt (line 5)) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from gdown->-r requirements.txt (line 5)) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from gdown->-r requirements.txt (line 5)) (2.31.0)\n",
      "Collecting absl-py>=0.4 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/7b/59/34dae935bbb42f3e8929c90e9dfff49090cef412cf767cf4f14cd01ded18/grpcio-1.68.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading grpcio-1.68.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from tb-nightly->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: packaging in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from tb-nightly->-r requirements.txt (line 6)) (23.0)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for protobuf!=4.24.0,>=3.19.6 from https://files.pythonhosted.org/packages/1c/f2/baf397f3dd1d3e4af7e3f5a0382b868d25ac068eefe1ebde05132333436c/protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from tb-nightly->-r requirements.txt (line 6)) (68.0.0)\n",
      "Requirement already satisfied: six>1.9 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from tb-nightly->-r requirements.txt (line 6)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tb-nightly->-r requirements.txt (line 6))\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from tb-nightly->-r requirements.txt (line 6)) (2.2.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 9)) (2.2.0)\n",
      "Requirement already satisfied: wcwidth in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from ftfy->-r requirements.txt (line 11)) (0.2.5)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from ogb>=1.2.6->wilds==1.2.2->-r requirements.txt (line 13)) (1.26.16)\n",
      "Collecting littleutils (from outdated>=0.2.0->wilds==1.2.2->-r requirements.txt (line 13))\n",
      "  Obtaining dependency information for littleutils from https://files.pythonhosted.org/packages/19/ac/a89d28d7421fffc028d68cdfde5e3e056e690cb4b1bbef4a5fea661e16f5/littleutils-0.2.4-py3-none-any.whl.metadata\n",
      "  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from pandas>=1.1.0->wilds==1.2.2->-r requirements.txt (line 13)) (2024.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tb-nightly->-r requirements.txt (line 6)) (2.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 5)) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.7.0->wilds==1.2.2->-r requirements.txt (line 13)) (1.3.0)\n",
      "Downloading flake8-3.7.9-py2.py3-none-any.whl (69 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m69.9/69.9 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading yapf-0.29.0-py2.py3-none-any.whl (185 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m185.3/185.3 kB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.3/42.3 kB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading wilds-1.2.2-py3-none-any.whl (92 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m92.5/92.5 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading tb_nightly-2.19.0a20241121-py3-none-any.whl (5.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.8/44.8 kB\u001B[0m \u001B[31m2.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.7/133.7 kB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m0m\n",
      "\u001B[?25hDownloading entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Downloading grpcio-1.68.0-cp311-cp311-macosx_10_9_universal2.whl (11.2 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.2/11.2 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.8/78.8 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m414.7/414.7 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\n",
      "\u001B[?25hDownloading pycodestyle-2.5.0-py2.py3-none-any.whl (51 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m51.2/51.2 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading pyflakes-2.1.1-py2.py3-none-any.whl (59 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m60.0/60.0 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\n",
      "Installing collected packages: yapf, mccabe, yacs, tensorboard-data-server, pyflakes, pycodestyle, protobuf, littleutils, isort, grpcio, ftfy, entrypoints, absl-py, tb-nightly, outdated, flake8, ogb, gdown, wilds\n",
      "  Attempting uninstall: yapf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Found existing installation: yapf 0.31.0\n",
      "    Uninstalling yapf-0.31.0:\n",
      "      Successfully uninstalled yapf-0.31.0\n",
      "  Attempting uninstall: mccabe\n",
      "    Found existing installation: mccabe 0.7.0\n",
      "    Uninstalling mccabe-0.7.0:\n",
      "      Successfully uninstalled mccabe-0.7.0\n",
      "  Attempting uninstall: pyflakes\n",
      "    Found existing installation: pyflakes 3.0.1\n",
      "    Uninstalling pyflakes-3.0.1:\n",
      "      Successfully uninstalled pyflakes-3.0.1\n",
      "  Attempting uninstall: pycodestyle\n",
      "    Found existing installation: pycodestyle 2.10.0\n",
      "    Uninstalling pycodestyle-2.10.0:\n",
      "      Successfully uninstalled pycodestyle-2.10.0\n",
      "  Attempting uninstall: isort\n",
      "    Found existing installation: isort 5.9.3\n",
      "    Uninstalling isort-5.9.3:\n",
      "      Successfully uninstalled isort-5.9.3\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.4\n",
      "    Uninstalling entrypoints-0.4:\n",
      "      Successfully uninstalled entrypoints-0.4\n",
      "  Attempting uninstall: flake8\n",
      "    Found existing installation: flake8 6.0.0\n",
      "    Uninstalling flake8-6.0.0:\n",
      "      Successfully uninstalled flake8-6.0.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "autopep8 1.6.0 requires pycodestyle>=2.8.0, but you have pycodestyle 2.5.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed absl-py-2.1.0 entrypoints-0.3 flake8-3.7.9 ftfy-6.3.1 gdown-5.2.0 grpcio-1.68.0 isort-4.3.21 littleutils-0.2.4 mccabe-0.6.1 ogb-1.3.6 outdated-0.2.2 protobuf-5.28.3 pycodestyle-2.5.0 pyflakes-2.1.1 tb-nightly-2.19.0a20241121 tensorboard-data-server-0.7.2 wilds-1.2.2 yacs-0.1.8 yapf-0.29.0\n",
      "/Users/greatpark/Desktop/Studies/3-2/Deep Learning/Homework/HW4/ProMetaR\n",
      "Collecting ftfy==6.1.1 (from -r requirements.txt (line 1))\n",
      "  Obtaining dependency information for ftfy==6.1.1 from https://files.pythonhosted.org/packages/e1/1e/bf736f9576a8979752b826b75cbd83663ff86634ea3055a766e2d8ad3ee5/ftfy-6.1.1-py3-none-any.whl.metadata\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: regex in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (4.65.0)\n",
      "Collecting learn2learn==0.2.0 (from -r requirements.txt (line 4))\n",
      "  Downloading learn2learn-0.2.0.tar.gz (7.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.0/7.0 MB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: wcwidth>=0.2.5 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from ftfy==6.1.1->-r requirements.txt (line 1)) (0.2.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.23.5)\n",
      "Collecting gym>=0.14.0 (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m721.7/721.7 kB\u001B[0m \u001B[31m2.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: torch>=1.1.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (0.15.1)\n",
      "Requirement already satisfied: scipy in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: requests in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from learn2learn==0.2.0->-r requirements.txt (line 4)) (2.31.0)\n",
      "Collecting gsutil (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading gsutil-5.31.tar.gz (3.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.0/3.0 MB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting qpth>=0.0.15 (from learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading qpth-0.0.18.tar.gz (16 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.2.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym>=0.14.0->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for gym_notices>=0.0.4 from https://files.pythonhosted.org/packages/25/26/d786c6bec30fe6110fd3d22c9a273a2a0e56c0b73b93e25ea1af5a53243b/gym_notices-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting cvxpy>=1.1.0 (from qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for cvxpy>=1.1.0 from https://files.pythonhosted.org/packages/9d/b0/19028903198cff00a03031aeb7fd4753e4379951e4a48986ef7e3503ff53/cvxpy-1.6.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading cvxpy-1.6.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from torchvision>=0.3.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (9.4.0)\n",
      "Collecting argcomplete>=1.9.4 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for argcomplete>=1.9.4 from https://files.pythonhosted.org/packages/f7/be/a606a6701d491cfae75583c80a6583f8abe9c36c0b9666e867e7cdd62fe8/argcomplete-3.5.1-py3-none-any.whl.metadata\n",
      "  Downloading argcomplete-3.5.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting crcmod>=1.7 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m89.7/89.7 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting fasteners>=0.14.1 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for fasteners>=0.14.1 from https://files.pythonhosted.org/packages/61/bf/fd60001b3abc5222d8eaa4a204cd8c0ae78e75adc688f33ce4bf25b7fafa/fasteners-0.19-py3-none-any.whl.metadata\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting gcs-oauth2-boto-plugin>=3.2 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading gcs-oauth2-boto-plugin-3.2.tar.gz (22 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting google-apitools>=0.5.32 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for google-apitools>=0.5.32 from https://files.pythonhosted.org/packages/5e/cb/cb0311f2ec371c83d6510847476c665edc9cc97564a51923557bc8f0b680/google_apitools-0.5.32-py3-none-any.whl.metadata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_apitools-0.5.32-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httplib2==0.20.4 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for httplib2==0.20.4 from https://files.pythonhosted.org/packages/59/0f/29725a9caf4b2618f524e0f28e2bda91aca8f880123ec77426ede6ea1ea4/httplib2-0.20.4-py3-none-any.whl.metadata\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting google-reauth>=0.1.0 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for google-reauth>=0.1.0 from https://files.pythonhosted.org/packages/69/e1/67ffaa3a645b86318ce30717af7145070ebccec5eef5c623ae08b86129b8/google_reauth-0.1.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_reauth-0.1.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting monotonic>=1.4 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for monotonic>=1.4 from https://files.pythonhosted.org/packages/9a/67/7e8406a29b6c45be7af7740456f7f37025f0506ae2e05fb9009a53946860/monotonic-1.6-py2.py3-none-any.whl.metadata\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=0.13 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (23.2.0)\n",
      "Collecting retry_decorator>=1.0.0 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading retry_decorator-1.1.1.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: six>=1.16.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting google-auth[aiohttp]==2.17.0 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for google-auth[aiohttp]==2.17.0 from https://files.pythonhosted.org/packages/b9/e6/d64f9f31c78fc405e82fdadc4e49b42a642cd3927a60b2fd063acd468a02/google_auth-2.17.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.17.0-py2.py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting google-auth-httplib2>=0.2.0 (from gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for google-auth-httplib2>=0.2.0 from https://files.pythonhosted.org/packages/be/8a/fe34d2f3f9470a27b01c9e76226965863f153d5fbe276f83608562e49c04/google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a4/07/14f8ad37f2d12a5ce41206c21820d8cb6561b728e51fad4530dff0552a67/cachetools-5.5.0-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0dev,>=3.6.2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.8.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from httplib2==0.20.4->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from requests->learn2learn==0.2.0->-r requirements.txt (line 4)) (2023.7.22)\n",
      "Collecting osqp>=0.6.2 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for osqp>=0.6.2 from https://files.pythonhosted.org/packages/cd/6d/0d17e8fa61809c125f97685d86e6cd6f7b1e745e01b8d3f96d783c8de41b/osqp-0.6.7.post3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading osqp-0.6.7.post3-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.9 kB)\n",
      "Collecting clarabel>=0.5.0 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for clarabel>=0.5.0 from https://files.pythonhosted.org/packages/8b/b9/e41f5316a2d4261c340d9fa6aa1694dd57d12cc45f1e5dfc5773d2b53d39/clarabel-0.9.0-cp37-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata\n",
      "  Downloading clarabel-0.9.0-cp37-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (4.8 kB)\n",
      "Collecting scs>=3.2.4.post1 (from cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for scs>=3.2.4.post1 from https://files.pythonhosted.org/packages/6e/6c/ee6f4b890b925f9a4696185c5b8c01cce0adbdef1f823662525196a5149c/scs-3.2.7-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading scs-3.2.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/e9/93/0c0f002031f18b53af7a6166103c02b9c0667be528944137cc954ec921b3/rsa-4.7.2-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting boto>=2.29.1 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for boto>=2.29.1 from https://files.pythonhosted.org/packages/23/10/c0b78c27298029e4454a472a1919bde20cb182dab1662cec7f2ca1dcc523/boto-2.49.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading boto-2.49.0-py2.py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting oauth2client>=2.2.0 (from gcs-oauth2-boto-plugin>=3.2->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for oauth2client>=2.2.0 from https://files.pythonhosted.org/packages/95/a9/4f25a14d23f0786b64875b91784607c2277eff25d48f915e39ff0cff505a/oauth2client-4.1.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (0.4.8)\n",
      "Collecting pyu2f (from google-reauth>=0.1.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Downloading pyu2f-0.1.5.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (41.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.1.0->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0.0dev,>=3.6.2->google-auth[aiohttp]==2.17.0->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (1.15.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qdldl (from osqp>=0.6.2->cvxpy>=1.1.0->qpth>=0.0.15->learn2learn==0.2.0->-r requirements.txt (line 4))\n",
      "  Obtaining dependency information for qdldl from https://files.pythonhosted.org/packages/b6/80/03426746bbf1151cef417f1cbd525375e358199620fea50af6f2d994b8d3/qdldl-0.1.7.post4-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading qdldl-0.1.7.post4-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pycparser in /Users/greatpark/anaconda3/lib/python3.11/site-packages (from cffi>=1.12->cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyOpenSSL>=0.13->gsutil->learn2learn==0.2.0->-r requirements.txt (line 4)) (2.21)\n",
      "Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.1/53.1 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.6/96.6 kB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading argcomplete-3.5.1-py3-none-any.whl (43 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.5/43.5 kB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading cvxpy-1.6.0-cp311-cp311-macosx_10_9_universal2.whl (1.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading google_auth-2.17.0-py2.py3-none-any.whl (178 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m178.1/178.1 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Downloading google_apitools-0.5.32-py3-none-any.whl (135 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m135.7/135.7 kB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading google_reauth-0.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n",
      "\u001B[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading clarabel-0.9.0-cp37-abi3-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (1.7 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n",
      "\u001B[?25hDownloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m98.2/98.2 kB\u001B[0m \u001B[31m4.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading osqp-0.6.7.post3-cp311-cp311-macosx_11_0_arm64.whl (237 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m237.6/237.6 kB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\n",
      "\u001B[?25hDownloading scs-3.2.7-cp311-cp311-macosx_11_0_arm64.whl (93 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m93.5/93.5 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading qdldl-0.1.7.post4-cp311-cp311-macosx_11_0_arm64.whl (99 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.7/99.7 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hBuilding wheels for collected packages: learn2learn, gym, qpth, gsutil, crcmod, gcs-oauth2-boto-plugin, retry_decorator, pyu2f\n",
      "  Building wheel for learn2learn (setup.py) ... \u001B[?25lerror\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\n",
      "  \n",
      "  \u001B[31m×\u001B[0m \u001B[32mpython setup.py bdist_wheel\u001B[0m did not run successfully.\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[171 lines of output]\u001B[0m\n",
      "  \u001B[31m   \u001B[0m /Users/greatpark/anaconda3/lib/python3.11/site-packages/setuptools/__init__.py:84: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  \u001B[31m   \u001B[0m !!\n",
      "  \u001B[31m   \u001B[0m \n",
      "  \u001B[31m   \u001B[0m         ********************************************************************************\n",
      "  \u001B[31m   \u001B[0m         Requirements should be satisfied by a PEP 517 installer.\n",
      "  \u001B[31m   \u001B[0m         If you are using pip, you can try `pip install --use-pep517`.\n",
      "  \u001B[31m   \u001B[0m         ********************************************************************************\n",
      "  \u001B[31m   \u001B[0m \n",
      "  \u001B[31m   \u001B[0m !!\n",
      "  \u001B[31m   \u001B[0m   dist.fetch_build_eggs(dist.setup_requires)\n",
      "  \u001B[31m   \u001B[0m running bdist_wheel\n",
      "  \u001B[31m   \u001B[0m running build\n",
      "  \u001B[31m   \u001B[0m running build_py\n",
      "  \u001B[31m   \u001B[0m creating build\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/_version.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/tests\n",
      "  \u001B[31m   \u001B[0m copying tests/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/tests\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/misc.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/kroneckers.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/protonet.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/metaoptnet.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/gbml.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/maml.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/meta_sgd.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/base_learner.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/learnable_optimizer.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/parameter_update.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/utils\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/utils/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/utils\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/utils/lightning.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/utils\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/transforms.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/async_vec_env.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/text\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/text/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/text\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/data\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/data/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/data\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/data/utils.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/data\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/data/samplers.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/data\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn/metalayers\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/metalayers/metamodule.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn/metalayers\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/metalayers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn/metalayers\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/nn/metalayers/parameter_transform.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/nn/metalayers\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms/lightning\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/lightning/lightning_episodic_module.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms/lightning\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/lightning/lightning_protonet.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms/lightning\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/lightning/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms/lightning\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/lightning/lightning_metaoptnet.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms/lightning\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/lightning/lightning_maml.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms/lightning\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/algorithms/lightning/lightning_anil.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/algorithms/lightning\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/transforms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/transforms/module_transform.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/transforms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/transforms/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/transforms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/transforms/kronecker_transform.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/transforms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/transforms/transform_dictionary.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/transforms\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/transforms/metacurvature_transform.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/transforms\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/update_rules\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/update_rules/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/update_rules\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/optim/update_rules/differentiable_sgd.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/optim/update_rules\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/fc100.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/mini_imagenet.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/vgg_flowers.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/fgvc_fungi.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/cifarfs.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/describable_textures.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/full_omniglot.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/tiered_imagenet.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/cu_birds200.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/fgvc_aircraft.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/datasets/quickdraw.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/datasets\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/models\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/models/resnet12.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/models\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/models/wrn28.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/models\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/models/cnn4.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/models\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/models\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/benchmarks\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/benchmarks\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/benchmarks/mini_imagenet_benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/benchmarks\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/benchmarks/fc100_benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/benchmarks\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/benchmarks/omniglot_benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/benchmarks\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/benchmarks/tiered_imagenet_benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/benchmarks\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/vision/benchmarks/cifarfs_benchmark.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/vision/benchmarks\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/meta_env.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/subproc_vec_env.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/mujoco/halfcheetah_forward_backward.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/mujoco/humanoid_forward_backward.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/mujoco/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/mujoco/ant_direction.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/mujoco/humanoid_direction.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/mujoco/ant_forward_backward.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/mujoco/dummy_mujoco_env.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/mujoco\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/metaworld\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/metaworld/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/metaworld\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/metaworld/metaworld.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/metaworld\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/particles\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/particles/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/particles\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/gym/envs/particles/particles_2d.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/gym/envs/particles\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/learn2learn/text/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/text/datasets/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/text/datasets\n",
      "  \u001B[31m   \u001B[0m copying learn2learn/text/datasets/news_classification.py -> build/lib.macosx-11.1-arm64-cpython-311/learn2learn/text/datasets\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/tests/unit\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/utils_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/tests/integration\n",
      "  \u001B[31m   \u001B[0m copying tests/integration/maml_omniglot_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/integration\n",
      "  \u001B[31m   \u001B[0m copying tests/integration/maml_miniimagenet_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/integration\n",
      "  \u001B[31m   \u001B[0m copying tests/integration/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/integration\n",
      "  \u001B[31m   \u001B[0m copying tests/integration/protonets_miniimagenet_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/integration\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/tests/unit/nn\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/nn/misc.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/nn\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/nn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/nn\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/nn/kroneckers_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/nn\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/nn/protonet_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/nn\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/nn/metaoptnet_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/nn\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/lightning_protonet_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/metasgd_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/maml_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/lightning_anil_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/lightning_metaoptnet_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/gbml_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/algorithms/lightning_maml_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/algorithms\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/transform_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/quickdraw_test_no.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/pretrained_backbones_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/fc100_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/benchmarks_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/vgg_flowers_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/fgvc_aircraft_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/tiered_imagenet_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/cu_birds200_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/cifarfs_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/vision/describable_textures_test_notravis.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/vision\n",
      "  \u001B[31m   \u001B[0m creating build/lib.macosx-11.1-arm64-cpython-311/tests/unit/data\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/data/util_datasets.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/data\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/data/utils_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/data\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/data/transforms_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/data\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/data/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/data\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/data/metadataset_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/data\n",
      "  \u001B[31m   \u001B[0m copying tests/unit/data/task_dataset_test.py -> build/lib.macosx-11.1-arm64-cpython-311/tests/unit/data\n",
      "  \u001B[31m   \u001B[0m running build_ext\n",
      "  \u001B[31m   \u001B[0m building 'learn2learn.data.meta_dataset' extension\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-11.1-arm64-cpython-311\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-11.1-arm64-cpython-311/learn2learn\n",
      "  \u001B[31m   \u001B[0m creating build/temp.macosx-11.1-arm64-cpython-311/learn2learn/data\n",
      "  \u001B[31m   \u001B[0m clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/greatpark/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/greatpark/anaconda3/include -arch arm64 -I/Users/greatpark/anaconda3/include/python3.11 -c learn2learn/data/meta_dataset.c -o build/temp.macosx-11.1-arm64-cpython-311/learn2learn/data/meta_dataset.o\n",
      "  \u001B[31m   \u001B[0m learn2learn/data/meta_dataset.c:210:12: fatal error: 'longintrepr.h' file not found\n",
      "  \u001B[31m   \u001B[0m   #include \"longintrepr.h\"\n",
      "  \u001B[31m   \u001B[0m            ^~~~~~~~~~~~~~~\n",
      "  \u001B[31m   \u001B[0m 1 error generated.\n",
      "  \u001B[31m   \u001B[0m error: command '/usr/bin/clang' failed with exit code 1\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\n",
      "  \n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001B[31m  ERROR: Failed building wheel for learn2learn\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[?25h  Running setup.py clean for learn2learn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for gym (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827627 sha256=928dcb192c3811507c3c48398804ca3e207875e8bf99dbdca96260ba71d92506\n",
      "  Stored in directory: /Users/greatpark/Library/Caches/pip/wheels/1c/77/9e/9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "  Building wheel for qpth (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for qpth: filename=qpth-0.0.18-py3-none-any.whl size=19552 sha256=4b3ab7a19e2188c355555bf731acb4af8e16fc0d7b15429a5641d9e56e1fe0ca\n",
      "  Stored in directory: /Users/greatpark/Library/Caches/pip/wheels/be/b5/f4/12c3bdf837e1564e4633463f0628e02ef101c6c2e198fc85b9\n",
      "  Building wheel for gsutil (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for gsutil: filename=gsutil-5.31-py3-none-any.whl size=3789454 sha256=251e675ac73bfb5760db1607743acfd22daabc4e4b2ee2d9ce9900a77ff38577\n",
      "  Stored in directory: /Users/greatpark/Library/Caches/pip/wheels/35/31/f5/3f1d2713377f3ac0770c5ee33d8f3e65e4a8718ba31c252a73\n",
      "  Building wheel for crcmod (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp311-cp311-macosx_11_0_arm64.whl size=21672 sha256=d39a6380742f36c301d366d8dc29ead72be9cbf29ed6dc9377ebdb52085a8a5e\n",
      "  Stored in directory: /Users/greatpark/Library/Caches/pip/wheels/23/94/7a/8cb7d14597e6395ce969933f01aed9ea8fa5f5b4d4c8a61e99\n",
      "  Building wheel for gcs-oauth2-boto-plugin (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for gcs-oauth2-boto-plugin: filename=gcs_oauth2_boto_plugin-3.2-py3-none-any.whl size=24473 sha256=9b2cf3e6ea35dfeeb68eb9eca1c560e0280e6eb6df9c26a1b0eba9ba3f8904ff\n",
      "  Stored in directory: /Users/greatpark/Library/Caches/pip/wheels/a0/89/64/75e825f5a502d767cbf63b3967be62bd28ee4aeb141f2cef04\n",
      "  Building wheel for retry_decorator (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for retry_decorator: filename=retry_decorator-1.1.1-py2.py3-none-any.whl size=3637 sha256=524077384c31416619e1ac29cfb9bf3ac89c905e54c5c5b35a5578875b0a56ec\n",
      "  Stored in directory: /Users/greatpark/Library/Caches/pip/wheels/42/60/35/e28d9a08360ed3f21670e4e9a0931c313541d432524840c018\n",
      "  Building wheel for pyu2f (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25h  Created wheel for pyu2f: filename=pyu2f-0.1.5-py3-none-any.whl size=39402 sha256=93ac8b0ba9968d254f663d9dd5e5f0d05f76965313a474fed4028e2bb060b6a4\n",
      "  Stored in directory: /Users/greatpark/Library/Caches/pip/wheels/e1/20/44/e0ce87d159f91c02f4f363be32614cb05ad9023f2ad2e63e82\n",
      "Successfully built gym qpth gsutil crcmod gcs-oauth2-boto-plugin retry_decorator pyu2f\n",
      "Failed to build learn2learn\n",
      "\u001B[31mERROR: Could not build wheels for learn2learn, which is required to install pyproject.toml-based projects\u001B[0m\u001B[31m\n",
      "\u001B[0m/Users/greatpark/Desktop/Studies/3-2/Deep Learning/Homework/HW4/ProMetaR/data\n",
      "zsh:1: command not found: wget\n",
      "unzip:  cannot find or open EuroSAT.zip, EuroSAT.zip.zip or EuroSAT.zip.ZIP.\n",
      "/Users/greatpark/Desktop/Studies/3-2/Deep Learning/Homework/HW4/ProMetaR/data/eurosat\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
      "To: /Users/greatpark/Desktop/Studies/3-2/Deep Learning/Homework/HW4/ProMetaR/data/eurosat/split_zhou_EuroSAT.json\n",
      "100%|██████████████████████████████████████| 3.01M/3.01M [00:00<00:00, 3.85MB/s]\n",
      "/Users/greatpark/Desktop/Studies/3-2/Deep Learning/Homework/HW4/ProMetaR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|████▋                                 | 43.1M/351M [00:12<01:20, 3.82MiB/s]"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mlvlab/ProMetaR.git\n",
    "%cd ProMetaR/\n",
    "\n",
    "!git clone https://github.com/KaiyangZhou/Dassl.pytorch.git\n",
    "%cd Dassl.pytorch/\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "!cp -r dassl ../\n",
    "# Install this library (no need to re-build if the source code is modified)\n",
    "# !python setup.py develop\n",
    "%cd ..\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "%mkdir outputs\n",
    "%mkdir data\n",
    "\n",
    "%cd data\n",
    "%mkdir eurosat\n",
    "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip EuroSAT.zip\n",
    "\n",
    "!unzip -o EuroSAT.zip -d eurosat/\n",
    "%cd eurosat\n",
    "!gdown 1Ip7yaCWFi0eaOFUGga0lUdVi_DDQth1o\n",
    "\n",
    "%cd ../../\n",
    "\n",
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import argparse\n",
    "from dassl.utils import setup_logger, set_random_seed, collect_env_info\n",
    "from dassl.config import get_cfg_default\n",
    "from dassl.engine import build_trainer\n",
    "from dassl.engine import TRAINER_REGISTRY, TrainerX\n",
    "from dassl.metrics import compute_accuracy\n",
    "from dassl.utils import load_pretrained_weights, load_checkpoint\n",
    "from dassl.optim import build_optimizer, build_lr_scheduler\n",
    "\n",
    "# custom\n",
    "import datasets.oxford_pets\n",
    "import datasets.oxford_flowers\n",
    "import datasets.fgvc_aircraft\n",
    "import datasets.dtd\n",
    "import datasets.eurosat\n",
    "import datasets.stanford_cars\n",
    "import datasets.food101\n",
    "import datasets.sun397\n",
    "import datasets.caltech101\n",
    "import datasets.ucf101\n",
    "import datasets.imagenet\n",
    "import datasets.imagenet_sketch\n",
    "import datasets.imagenetv2\n",
    "import datasets.imagenet_a\n",
    "import datasets.imagenet_r\n",
    "\n",
    "def print_args(args, cfg):\n",
    "    print(\"***************\")\n",
    "    print(\"** Arguments **\")\n",
    "    print(\"***************\")\n",
    "    optkeys = list(args.__dict__.keys())\n",
    "    optkeys.sort()\n",
    "    for key in optkeys:\n",
    "        print(\"{}: {}\".format(key, args.__dict__[key]))\n",
    "    print(\"************\")\n",
    "    print(\"** Config **\")\n",
    "    print(\"************\")\n",
    "    print(cfg)\n",
    "\n",
    "def reset_cfg(cfg, args):\n",
    "    if args.root:\n",
    "        cfg.DATASET.ROOT = args.root\n",
    "    if args.output_dir:\n",
    "        cfg.OUTPUT_DIR = args.output_dir\n",
    "    if args.seed:\n",
    "        cfg.SEED = args.seed\n",
    "    if args.trainer:\n",
    "        cfg.TRAINER.NAME = args.trainer\n",
    "    cfg.DATASET.NUM_SHOTS = 16\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = args.subsample_classes\n",
    "    cfg.DATALOADER.TRAIN_X.BATCH_SIZE = args.train_batch_size\n",
    "    cfg.OPTIM.MAX_EPOCH = args.epoch\n",
    "\n",
    "def extend_cfg(cfg):\n",
    "    \"\"\"\n",
    "    Add new config variables.\n",
    "    \"\"\"\n",
    "    from yacs.config import CfgNode as CN\n",
    "    cfg.TRAINER.COOP = CN()\n",
    "    cfg.TRAINER.COOP.N_CTX = 16  # number of context vectors\n",
    "    cfg.TRAINER.COOP.CSC = False  # class-specific context\n",
    "    cfg.TRAINER.COOP.CTX_INIT = \"\"  # initialization words\n",
    "    cfg.TRAINER.COOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.COOP.CLASS_TOKEN_POSITION = \"end\"  # 'middle' or 'end' or 'front'\n",
    "    cfg.TRAINER.COCOOP = CN()\n",
    "    cfg.TRAINER.COCOOP.N_CTX = 4  # number of context vectors\n",
    "    cfg.TRAINER.COCOOP.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.COCOOP.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR = CN()\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_VISION = 4  # number of context vectors at the vision branch\n",
    "    cfg.TRAINER.PROMETAR.N_CTX_TEXT = 4  # number of context vectors at the language branch\n",
    "    cfg.TRAINER.PROMETAR.CTX_INIT = \"a photo of a\"  # initialization words\n",
    "    cfg.TRAINER.PROMETAR.PREC = \"fp16\"  # fp16, fp32, amp\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_VISION = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.TRAINER.PROMETAR.PROMPT_DEPTH_TEXT = 9  # Max 12, minimum 0, for 0 it will be using shallow IVLP prompting (J=1)\n",
    "    cfg.DATASET.SUBSAMPLE_CLASSES = \"all\"  # all, base or new\n",
    "    cfg.TRAINER.PROMETAR.ADAPT_LR = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.LR_RATIO = 0.0005\n",
    "    cfg.TRAINER.PROMETAR.FAST_ADAPTATION = False\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_ALPHA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.MIXUP_BETA = 0.5\n",
    "    cfg.TRAINER.PROMETAR.DIM_RATE=8\n",
    "    cfg.OPTIM_VNET = CN()\n",
    "    cfg.OPTIM_VNET.NAME = \"adam\"\n",
    "    cfg.OPTIM_VNET.LR = 0.0003\n",
    "    cfg.OPTIM_VNET.WEIGHT_DECAY = 5e-4\n",
    "    cfg.OPTIM_VNET.MOMENTUM = 0.9\n",
    "    cfg.OPTIM_VNET.SGD_DAMPNING = 0\n",
    "    cfg.OPTIM_VNET.SGD_NESTEROV = False\n",
    "    cfg.OPTIM_VNET.RMSPROP_ALPHA = 0.99\n",
    "    cfg.OPTIM_VNET.ADAM_BETA1 = 0.9\n",
    "    cfg.OPTIM_VNET.ADAM_BETA2 = 0.999\n",
    "    cfg.OPTIM_VNET.STAGED_LR = False\n",
    "    cfg.OPTIM_VNET.NEW_LAYERS = ()\n",
    "    cfg.OPTIM_VNET.BASE_LR_MULT = 0.1\n",
    "    # Learning rate scheduler\n",
    "    cfg.OPTIM_VNET.LR_SCHEDULER = \"single_step\"\n",
    "    # -1 or 0 means the stepsize is equal to max_epoch\n",
    "    cfg.OPTIM_VNET.STEPSIZE = (-1, )\n",
    "    cfg.OPTIM_VNET.GAMMA = 0.1\n",
    "    cfg.OPTIM_VNET.MAX_EPOCH = 10\n",
    "    # Set WARMUP_EPOCH larger than 0 to activate warmup training\n",
    "    cfg.OPTIM_VNET.WARMUP_EPOCH = -1\n",
    "    # Either linear or constant\n",
    "    cfg.OPTIM_VNET.WARMUP_TYPE = \"linear\"\n",
    "    # Constant learning rate when type=constant\n",
    "    cfg.OPTIM_VNET.WARMUP_CONS_LR = 1e-5\n",
    "    # Minimum learning rate when type=linear\n",
    "    cfg.OPTIM_VNET.WARMUP_MIN_LR = 1e-5\n",
    "    # Recount epoch for the next scheduler (last_epoch=-1)\n",
    "    # Otherwise last_epoch=warmup_epoch\n",
    "    cfg.OPTIM_VNET.WARMUP_RECOUNT = True\n",
    "\n",
    "def setup_cfg(args):\n",
    "    cfg = get_cfg_default()\n",
    "    extend_cfg(cfg)\n",
    "    # 1. From the dataset config file\n",
    "    if args.dataset_config_file:\n",
    "        cfg.merge_from_file(args.dataset_config_file)\n",
    "    # 2. From the method config file\n",
    "    if args.config_file:\n",
    "        cfg.merge_from_file(args.config_file)\n",
    "    # 3. From input arguments\n",
    "    reset_cfg(cfg, args)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "def load_clip_to_cpu(cfg): # Load CLIP\n",
    "    backbone_name = cfg.MODEL.BACKBONE.NAME\n",
    "    url = clip._MODELS[backbone_name]\n",
    "    model_path = clip._download(url)\n",
    "\n",
    "    try:\n",
    "        # loading JIT archive\n",
    "        model = torch.jit.load(model_path, map_location=\"cpu\").eval()\n",
    "        state_dict = None\n",
    "\n",
    "    except RuntimeError:\n",
    "        state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "\n",
    "    if cfg.TRAINER.NAME == \"\":\n",
    "      design_trainer = \"CoOp\"\n",
    "    else:\n",
    "      design_trainer = cfg.TRAINER.NAME\n",
    "    design_details = {\"trainer\": design_trainer,\n",
    "                      \"vision_depth\": 0,\n",
    "                      \"language_depth\": 0, \"vision_ctx\": 0,\n",
    "                      \"language_ctx\": 0}\n",
    "    model = clip.build_model(state_dict or model.state_dict(), design_details)\n",
    "\n",
    "    return model\n",
    "\n",
    "from dassl.config import get_cfg_default\n",
    "cfg = get_cfg_default()\n",
    "cfg.MODEL.BACKBONE.NAME = \"ViT-B/16\" # Set the vision encoder backbone of CLIP to ViT.\n",
    "clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model): # 초기화 하는 함수\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts): # 모델 호출\n",
    "        x = prompts + self.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "@TRAINER_REGISTRY.register(force=True)\n",
    "class CoCoOp(TrainerX):\n",
    "    def check_cfg(self, cfg):\n",
    "        assert cfg.TRAINER.COCOOP.PREC in [\"fp16\", \"fp32\", \"amp\"]\n",
    "\n",
    "    def build_model(self):\n",
    "        cfg = self.cfg\n",
    "        classnames = self.dm.dataset.classnames\n",
    "        print(f\"Loading CLIP (backbone: {cfg.MODEL.BACKBONE.NAME})\")\n",
    "        clip_model = load_clip_to_cpu(cfg)\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp32\" or cfg.TRAINER.COCOOP.PREC == \"amp\":\n",
    "            # CLIP's default precision is fp16\n",
    "            clip_model.float()\n",
    "\n",
    "        print(\"Building custom CLIP\")\n",
    "        self.model = CoCoOpCustomCLIP(cfg, classnames, clip_model)\n",
    "\n",
    "        print(\"Turning off gradients in both the image and the text encoder\")\n",
    "        name_to_update = \"prompt_learner\"\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name_to_update not in name:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        # Double check\n",
    "        enabled = set()\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                enabled.add(name)\n",
    "        print(f\"Parameters to be updated: {enabled}\")\n",
    "\n",
    "        if cfg.MODEL.INIT_WEIGHTS:\n",
    "            load_pretrained_weights(self.model.prompt_learner, cfg.MODEL.INIT_WEIGHTS)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # NOTE: only give prompt_learner to the optimizer\n",
    "        self.optim = build_optimizer(self.model.prompt_learner, cfg.OPTIM)\n",
    "        self.sched = build_lr_scheduler(self.optim, cfg.OPTIM)\n",
    "        self.register_model(\"prompt_learner\", self.model.prompt_learner, self.optim, self.sched)\n",
    "\n",
    "        self.scaler = GradScaler() if cfg.TRAINER.COCOOP.PREC == \"amp\" else None\n",
    "\n",
    "        # Note that multi-gpu training could be slow because CLIP's size is\n",
    "        # big, which slows down the copy operation in DataParallel\n",
    "        device_count = torch.cuda.device_count()\n",
    "        if device_count > 1:\n",
    "            print(f\"Multiple GPUs detected (n_gpus={device_count}), use all of them!\")\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def before_train(self):\n",
    "        directory = self.cfg.OUTPUT_DIR\n",
    "        if self.cfg.RESUME:\n",
    "            directory = self.cfg.RESUME\n",
    "        self.start_epoch = self.resume_model_if_exist(directory)\n",
    "\n",
    "        # Remember the starting time (for computing the elapsed time)\n",
    "        self.time_start = time.time()\n",
    "\n",
    "\n",
    "    def forward_backward(self, batch):\n",
    "        image, label = self.parse_batch_train(batch)\n",
    "\n",
    "        model = self.model\n",
    "        optim = self.optim\n",
    "        scaler = self.scaler\n",
    "\n",
    "        prec = self.cfg.TRAINER.COCOOP.PREC\n",
    "        loss = model(image, label) # Input image 모델 통과\n",
    "        optim.zero_grad()\n",
    "        loss.backward() # Backward (역전파)\n",
    "        optim.step() # 모델 parameter update\n",
    "\n",
    "        loss_summary = {\"loss\": loss.item()}\n",
    "\n",
    "        if (self.batch_idx + 1) == self.num_batches:\n",
    "            self.update_lr()\n",
    "\n",
    "        return loss_summary\n",
    "\n",
    "    def parse_batch_train(self, batch):\n",
    "        input = batch[\"img\"]\n",
    "        label = batch[\"label\"]\n",
    "        input = input.to(self.device)\n",
    "        label = label.to(self.device)\n",
    "        return input, label\n",
    "\n",
    "    def load_model(self, directory, epoch=None):\n",
    "        if not directory:\n",
    "            print(\"Note that load_model() is skipped as no pretrained model is given\")\n",
    "            return\n",
    "\n",
    "        names = self.get_model_names()\n",
    "\n",
    "        # By default, the best model is loaded\n",
    "        model_file = \"model-best.pth.tar\"\n",
    "\n",
    "        if epoch is not None:\n",
    "            model_file = \"model.pth.tar-\" + str(epoch)\n",
    "\n",
    "        for name in names:\n",
    "            model_path = osp.join(directory, name, model_file)\n",
    "\n",
    "            if not osp.exists(model_path):\n",
    "                raise FileNotFoundError('Model not found at \"{}\"'.format(model_path))\n",
    "\n",
    "            checkpoint = load_checkpoint(model_path)\n",
    "            state_dict = checkpoint[\"state_dict\"]\n",
    "            epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "            # Ignore fixed token vectors\n",
    "            if \"token_prefix\" in state_dict:\n",
    "                del state_dict[\"token_prefix\"]\n",
    "\n",
    "            if \"token_suffix\" in state_dict:\n",
    "                del state_dict[\"token_suffix\"]\n",
    "\n",
    "            print(\"Loading weights to {} \" 'from \"{}\" (epoch = {})'.format(name, model_path, epoch))\n",
    "            # set strict=False\n",
    "            self._models[name].load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def after_train(self):\n",
    "      print(\"Finish training\")\n",
    "\n",
    "      do_test = not self.cfg.TEST.NO_TEST\n",
    "      if do_test:\n",
    "          if self.cfg.TEST.FINAL_MODEL == \"best_val\":\n",
    "              print(\"Deploy the model with the best val performance\")\n",
    "              self.load_model(self.output_dir)\n",
    "          else:\n",
    "              print(\"Deploy the last-epoch model\")\n",
    "          acc = self.test()\n",
    "\n",
    "      # Show elapsed time\n",
    "      elapsed = round(time.time() - self.time_start)\n",
    "      elapsed = str(datetime.timedelta(seconds=elapsed))\n",
    "      print(f\"Elapsed: {elapsed}\")\n",
    "\n",
    "      # Close writer\n",
    "      self.close_writer()\n",
    "      return acc\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Generic training loops.\"\"\"\n",
    "        self.before_train()\n",
    "        for self.epoch in range(self.start_epoch, self.max_epoch):\n",
    "            self.before_epoch()\n",
    "            self.run_epoch()\n",
    "            self.after_epoch()\n",
    "        acc = self.after_train()\n",
    "        return acc\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--root\", type=str, default=\"data/\", help=\"path to dataset\")\n",
    "parser.add_argument(\"--output-dir\", type=str, default=\"outputs/cocoop3\", help=\"output directory\")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, help=\"only positive value enables a fixed seed\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config-file\", type=str, default=\"configs/trainers/ProMetaR/vit_b16_c2_ep10_batch4_4+4ctx.yaml\", help=\"path to config file\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset-config-file\",\n",
    "    type=str,\n",
    "    default=\"configs/datasets/eurosat.yaml\",\n",
    "    help=\"path to config file for dataset setup\",\n",
    ")\n",
    "parser.add_argument(\"--trainer\", type=str, default=\"CoOp\", help=\"name of trainer\")\n",
    "parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"evaluation only\")\n",
    "parser.add_argument(\n",
    "    \"--model-dir\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    "    help=\"load model from this directory for eval-only mode\",\n",
    ")\n",
    "parser.add_argument(\"--train-batch-size\", type=int, default=4)\n",
    "parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "parser.add_argument(\"--subsample-classes\", type=str, default=\"base\")\n",
    "parser.add_argument(\n",
    "    \"--load-epoch\", type=int, default=0, help=\"load model weights at this epoch for evaluation\"\n",
    ")\n",
    "args = parser.parse_args([])\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup_cfg(args)\n",
    "    if cfg.SEED >= 0:\n",
    "        set_random_seed(cfg.SEED)\n",
    "\n",
    "    if torch.cuda.is_available() and cfg.USE_CUDA:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    trainer = build_trainer(cfg)\n",
    "    if args.eval_only:\n",
    "        trainer.load_model(args.model_dir, epoch=args.load_epoch)\n",
    "        acc = trainer.test()\n",
    "        return acc\n",
    "\n",
    "    acc = trainer.train()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3n9blo4JO7m"
   },
   "source": [
    "### **Q1.  Understanding and implementing CoCoOp**\n",
    "* We have learned how to define CoOp in Lab Session 4.\n",
    "\n",
    "* The main difference between CoOp and CoCoOp is **meta network** to extract image tokens that is added to the text prompt.\n",
    "\n",
    "* Based on the CoOp code given in Lab Session 4, fill-in-the-blank exercise to test your understanding of critical parts of the CoCoOp.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SONlVIhPH_qF"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CoCoOpPromptLearner(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COCOOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COCOOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1: 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)  # Wrap the initialized prompts above as parameters to make them trainable.\n",
    "\n",
    "        ### Tokenize ###\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]  # 예) \"Forest\"\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames] # 예) \"A photo of Forest.\"\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]) # 예) [49406, 320, 1125, 539...]\n",
    "\n",
    "\n",
    "\n",
    "        #####################################\n",
    "        ####### Q1. Fill in the blank #######\n",
    "        ########## Define Meta Net ##########\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            #(\"linear1\", \"fill in here\"(vis_dim, vis_dim // 16)),\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "        #####################################\n",
    "        ## Hint: meta network is composed to linear layer, relu activation, and linear layer.\n",
    "\n",
    "\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp16\":\n",
    "            self.meta_net.half()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
    "\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx:, :])  # CLS, EOS\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
    "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
    "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
    "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
    "\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (dim0, 1, dim)\n",
    "                ctx,  # (dim0, n_ctx, dim)\n",
    "                suffix,  # (dim0, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx  # (n_ctx, ctx_dim)\n",
    "\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q2,3. Fill in the blank #########\n",
    "        #bias = self.meta_net(\"Fill in here, Hint: Image feature is given as input to meta network\")  # (batch, ctx_dim)\n",
    "        bias = self.meta_net(im_features)  # (batch, ctx_dim)\n",
    "        bias = bias.unsqueeze(1)  # (batch, 1, ctx_dim)\n",
    "        ctx = ctx.unsqueeze(0)  # (1, n_ctx, ctx_dim)\n",
    "        #ctx_shifted = ctx + \"Fill in here, Hint: Add meta token to context token\"  # (batch, n_ctx, ctx_dim)\n",
    "        ctx_shifted = ctx + bias  # (batch, n_ctx, ctx_dim)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "\n",
    "        # Use instance-conditioned context tokens for all classes\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "\n",
    "        return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_BluKEdKA94"
   },
   "outputs": [],
   "source": [
    "class CoCoOpCustomCLIP(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = CoCoOpPromptLearner(cfg, classnames, clip_model)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "        ############################################\n",
    "        ########## Q4. Fill in the blank #########\n",
    "        #prompts = self.prompt_learner(\"Fill in here\")\n",
    "        prompts = self.prompt_learner(image_features)\n",
    "        ############################################\n",
    "        ############################################\n",
    "\n",
    "\n",
    "        logits = []\n",
    "        for pts_i, imf_i in zip(prompts, image_features):\n",
    "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "            l_i = logit_scale * imf_i @ text_features.t()\n",
    "            logits.append(l_i)\n",
    "        logits = torch.stack(logits)\n",
    "\n",
    "        if self.prompt_learner.training:\n",
    "            return F.cross_entropy(logits, label)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CGZlqo-HtRN"
   },
   "source": [
    "### **Q2. Trainining CoCoOp**\n",
    "\n",
    "In this task, you will train CoCoOp on the EuroSAT dataset. If your implementation of CoCoOp in Question 1 is correct, the following code should execute without errors. Please submit the execution file so we can evaluate whether your code runs without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy3bAMnBMrXP"
   },
   "outputs": [],
   "source": [
    "# Train on the Base Classes Train split and evaluate accuracy on the Base Classes Test split.\n",
    "args.trainer = \"CoCoOp\"\n",
    "args.train_batch_size = 4\n",
    "args.epoch = 100\n",
    "args.output_dir = \"outputs/cocoop\"\n",
    "\n",
    "args.subsample_classes = \"base\"\n",
    "args.eval_only = False\n",
    "cocoop_base_acc = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xql7WpJ5vPII"
   },
   "outputs": [],
   "source": [
    "# Accuracy on the New Classes.\n",
    "args.model_dir = \"outputs/cocoop\"\n",
    "args.output_dir = \"outputs/cocoop/new_classes\"\n",
    "args.subsample_classes = \"new\"\n",
    "args.load_epoch = 100\n",
    "args.eval_only = True\n",
    "coop_novel_acc = main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1KdgiKFsowj"
   },
   "source": [
    "### **Q3. Analyzing the results of CoCoOp**\n",
    "Compare the results of CoCoOp with those of CoOp that we trained in Lab Session 4. Discuss possible reasons for the performance differences observed between CoCoOp and CoOp."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "G3n9blo4JO7m",
    "2CGZlqo-HtRN"
   ],
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
